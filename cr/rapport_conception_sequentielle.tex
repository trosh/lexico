\section{Conception séquentielle}

\subsection{Vue d'ensemble}

\paragraph{Problème donné}

On part dans notre projet d'un fichier
``type wikipedia'' qui représente un corpus de documents.
\begin{addmargin}{.5cm}
\begin{verbatim}
[[Document 1]]
Texte, texte
[[File:filename.ext|arg]]
Texte, texte
[[Document 2]]
Plus de texte
\end{verbatim}
\end{addmargin}

\paragraph{But}

On veut déterminer les champs lexicaux de ce corpus de document.

Par ceci on veut dire déterminer des groupes de mots
qui ont une utilisation (à défaut d'un sens, qu'on ne
peut pas déterminer automatiquement) similaire.
De même on veut pouvoir montrer comment des documents
sont liés ensemble.

On réduit le problème en sous-problèmes \----
tout d'abord on prépare les documents,
puis on crée une liste des mots avec occurences
d'un document, qu'on intègre à un dictionnaire existant.
Enfin on génère des matrices de distances à partir
desquelles on peut trouver des clusters de mots.

\paragraph{Conditions}

On veut faire aussi peu de suppositions sur les
documents et les mots que possible.

En effet on s'autorise certaines règles basiques pour
reconnaître un mot et des méthodes statistiques pour
faire ressortir les mots importants par leur usage
mais jamais par des listes prédéfinies.
Si besoin, et dans le meilleur des cas, on peut construire
des listes à partir des analyses, comme par apprentissage.

Notre problème doit tenir dans la mémoire,
car ce n'est pas dans le spectre du projet de gérer des accès
au fichiers en parallèle ou des grandes bases de données.

\subsection{Découpage des fichiers}

On appelle découpage d'un fichier la création d'une
liste des occurences de tous les mots qui y sont présents.
L'appel correspondant à ceci est la fonction {\tt decoupe\_fichier}
dans {\tt src/lexico.c}.

On fait ce découpage en une passe unique par document
où on remplit une zone de travail qu'on vide dans le tableau
d'occurence ou dans un nouveau mot selon les caractères lus
et une recherche dans le tableau.

On peut comme un exemple montrer la fonction
{\tt est\_une\_lettre\_valable} qui vérifie les caractères
un par un:

\begin{verbatim}
inline int est_une_lettre_valable(char c) {
    return ((c >= 'A' && c <= 'Z')   // EST UNE LETTRE MAJUSCULE
         || (c >= 'a' && c <= 'z')   // EST UNE LETTRE MINUSCULE
         ||  c == '-' || c == '\''); // EST UN HYPHEN / QUOTE
}
\end{verbatim}

\subsection{Préparation des fichiers}

On crée un autre programme, {\tt splitwiki}, qui transforme le
fichier d'entrée type wikipedia en de nombreux fichiers,
ce qui nous garantit 1 document = 1 fichier.
On se base sur plusieurs règles syntaxiques simples
donc l'exécution est très rapide.

On utilise le programme iconv pour translitérer du
texte utf8 en ASCII pour garantir autant que possible
et en particulier au niveau des mots 1 octet = 1 caractère.

Ces deux étapes qui doivent préceder le découpage
résultent de la simplicité avec laquelle on a fait
le découpage.

\subsection{Ajout au dictionnaire}

À chaque fois qu'un fichier est découpé en sa liste de mots
on ajoute celle-ci au dictionnaire global, qui est un tableau
de tableau dynamiquement alloués et réalloués.
Ceci correspond à des parcours de complexité souvent $n^2$
où l'on doit comparer des chaînes de caractères ensemble.
On réduit un peu le nombre de comparaisons en précalculant
des checksums.

On va ensuite préparer les matrices de mots/documents stockées
pleines 1D.

\subsection{Boucle itérative de calcul des distances}

En se basant l'une sur l'autre, on compare chaque mot avec
chaque autre mot et chaque document avec chaque autre
document pour calculer un score de distances entre les
mots/documents. On devrait avoir un score borné qui
représente une notion tangible mais là le calcul nous
a été donné sans explication et qui ne convergait pas
dans sa version originelle. On n'a pas réussi à aller plus
loin que ceci.
